---
title: "Data-Exploration"
author: "J. Welch"
date: "October 17, 2016"
output: 
  html_document: 
    keep_md: yes
---

# NCDC Storm Events Database Analysis

## Get Data from DATA.gov

I am an MBA student at Bridgewater State University, enrolled in MGMT582 - Business Intelligence/Analytics.  This data exploration project is intended to be my final project and in-class presentation.  The goal of this project is to gain practical working experience with R Studio and to apply some data analysis techniques in which we have been introduced within this course.

I have chosen to work with the NCDC Storm Events Database available at DATA.gov.  For more details related to this raw data set, see <https://catalog.data.gov/dataset/ncdc-storm-events-database>.

My first step in preparing my analysis will be to write a script to get the data at Data.gov.  After following instructions about the storm events database, I found that the raw data sits in zipped files located on an FTP server at <ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/>.  I have prepared a script that will automatically get this data from its FTP location, unzip each file, and then append it to create a larger file that can be analyzed here with R Studio.  The development of this script was an iterative process, as I needed to be cautious of memory requirements.  

With my first pass at the collection of data from Data.gov, I collected 10 years of data back to 2006.  I found that my personal computer could handle this task and the stormdata.csv file was approximately 200MB in size.  Since my computer has nearly 8GB of RAM, I decided to go ahead and collect a full 65 years of storm data.  After pulling 65 years of data, I observed that my final stormdata.csv file was only 400MB in size.  This appears to be a very manageable data set that can be conveniently explored here with R Studio.

Now, the process of collecting this data via FTP, downloading, unzipping, and then concatenating each of the individual files is quite timely and it is not necessary to do all of this work with each and every execution of the Knitr markdown script.  To aid in the data exploration process, I will introduce caching into R Studio with the script as follows:


```{r setup, include=TRUE}
knitr::opts_chunk$set(cache=TRUE)
```

Let's now iterate through 65 years of storm data, collect this data via download from the public FTP site, concatenate the data into a larger data set, and store a copy of this file locally on our computer.


```{r}
if (!file.exists("./stormdata.csv")) {
    # Load the R.utils library
    library("R.utils")
    
    i <- 1

    for (year in 1951:2016) {
        # Determine file name extension
        if (year==2006 || year==2014 || year==2015 || year==2016) {
            extstr <- "_c20161118.csv.gz"
        } else {
            extstr <- "_c20160223.csv.gz"
        }
        
        # Identify the URL of the download file
        URL <- paste("ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/StormEvents_details-ftp_v1.0_d", year, extstr, sep="")
        #print(URL)

        # Download the file into the current directory
        download.file(URL, destfile = "./temp.csv.gz")

        # Unzip the file
        gunzip("./temp.csv.gz")

        # Read the file into memory
        tempdata <- read.csv("./temp.csv")
        
        # Delete the lengthy narrative columns
        # Filesize with these columns is 272MB
        # Filesize without these columns 79MB
        tempdata$EPISODE_NARRATIVE <- NULL
        tempdata$EVENT_NARRATIVE <- NULL
    
        if (i==1) {
            stormdata <- tempdata
        } else {
            stormdata <- rbind(stormdata, tempdata)
        }

        # Delete the temporary file
        unlink("./temp.csv")
        
        # Write the loaded data to local CSV file
        write.csv(stormdata, "./stormdata.csv")
        
        # Increment i
        i <- i+1
    }
    
} else if (!exists("stormdata")) {
    stormdata <- read.csv("./stormdata.csv")
}

```
## Initial Table Exploration and Observation

The script above has successfully assembled 65 years of data from the NCDC Storm Events Database from calendar year 1951 through summer of 2016.  We have eliminated the long text fields related to the description of the storm events.  We did this in order to produce a table size which is manageable on our desktop computers.  Elimination of these text fields dropped the file size to nearly 25% of its original size.  Our final CSV file is appx 400MB in size and it contains 1,401,817 observations of 50 variables.

It is now desirable to conduct some exploration techniques as outlined on RDatamining.com.  We will follow the examples put forth on the following page in order to begin our exploration of this significant data file.

<http://www.rdatamining.com/examples/exploration>


```{r}
# View the data frame inside the Global Environment of R Studio
#View(stormdata)

# Check the dimensionality
dim(stormdata)

# Output the names of the columns
names(stormdata)

# List the structure of the file
str(stormdata)

# Output the first 5 rows of the file
head(stormdata, n=5)

# Distribution of every variable
summary(stormdata)

```

## Introduction to the Table() Function

According to R-Bloggers.com <https://www.r-bloggers.com/r-function-of-the-day-table/>, "the table() function is a very basic, but essential, function to master while performing interactive data analyses.  It simply creates tabular results of categorical variables.  However, when combined with the powers of logical expressions in R, you can gain even more insights into your data, including identifying potential problems.

```{r}
# Frequency of event types - sorted by decreasing count and selected for top 10 events.
counts <- sort(table(stormdata$EVENT_TYPE), decreasing=TRUE)[1:10]

# Output the first 5 rows of the file
head(counts, n=25)

# Get the names of columns on counts table
names(counts)

# Pie Chart
pie(counts)

# Simple Horizontal Bar Plot
barplot(counts, main="Storm Event Type", horiz=TRUE, xlab="Frequency", names.arg=names(counts), cex.names=0.5, las=1)

```

## Trend Analysis

It is now desirable to complete a trend analysis whereby we can visualize the frequency of events by year so that we can see if there is an upward trend in the number of occurrences of these major weather events.

I want to complete a sub-setting technique using R, such that I complete the equivalent of an SQL query like the following:

SELECT YEAR, COUNT(*) 
  FROM stormdata
  WHERE EVENT_TYPE='Thunderstorm Wind'
  GROUP BY YEAR
  ORDER BY YEAR
  
### Thunderstorm Events

```{r}
# Subset the stormdata data frame
thunderstorms <- stormdata[stormdata$EVENT_TYPE=="Thunderstorm Wind",c('YEAR', 'EVENT_TYPE', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'MAGNITUDE', 'MAGNITUDE_TYPE')] %>% group_by(YEAR) %>% summarise(number = n())

# Create Simple Plot
plot(thunderstorms$YEAR, thunderstorms$number, type="p", main="Thunderstorms", xlab="Year", ylab="Number of Events")

```

### Hail Storm Events

```{r}
# Subset the stormdata data frame
hailstorms <- stormdata[stormdata$EVENT_TYPE=="Hail",c('YEAR', 'EVENT_TYPE', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'MAGNITUDE', 'MAGNITUDE_TYPE')] %>% group_by(YEAR) %>% summarise(number = n())

# Create Simple Plot
plot(hailstorms$YEAR, hailstorms$number, type="p", main="Hail Storms", xlab="Year", ylab="Number of Events")

```

### Tornado Events

```{r}
# Subset the stormdata data frame
tornados <- stormdata[stormdata$EVENT_TYPE=="Tornado",c('YEAR', 'EVENT_TYPE', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'MAGNITUDE', 'MAGNITUDE_TYPE')] %>% group_by(YEAR) %>% summarise(number = n())

# Create Simple Plot
plot(tornados$YEAR, tornados$number, type="p", main="Tornados", xlab="Year", ylab="Number of Events")

```

### Winter Storm Events

```{r}
# Subset the stormdata data frame
winterstorms <- stormdata[stormdata$EVENT_TYPE=="Winter Storm",c('YEAR', 'EVENT_TYPE', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'MAGNITUDE', 'MAGNITUDE_TYPE')] %>% group_by(YEAR) %>% summarise(number = n())

# Create Simple Plot
plot(winterstorms$YEAR, winterstorms$number, type="p", main="Winter Storms", xlab="Year", ylab="Number of Events")

```

### Snow Storm Events

```{r}
# Subset the stormdata data frame
snowstorms <- stormdata[stormdata$EVENT_TYPE=="Heavy Snow",c('YEAR', 'EVENT_TYPE', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'MAGNITUDE', 'MAGNITUDE_TYPE')] %>% group_by(YEAR) %>% summarise(number = n())

# Create Simple Plot
plot(snowstorms$YEAR, snowstorms$number, type="p", main="Snow Storms", xlab="Year", ylab="Number of Events")

```

## Conclusions

This data analysis project has certainly exposed me to many tools and resources which are available in order to complete a data mining project using important historical data sets.  We have gathered a total of 65 years of weather data for this exercise from the United States government via Data.gov and we have explored the content of this data with the aid of R Studio.  We set out to see if we could create a visualization of this data that demonstrates claims that global warming could be contributing to an increased occurrence of weather events here within the United States.  The historical data certainly shows a significant increase in the occurrence of all weather events (with the only exception where there is limited historical data).  In fact, the records show that the increased frequency of storms is shockingly increasing at what appears to be logarithmic rates.  What we don't know from this data is whether or not the sharp upward trend is a reflection of our increased ability to monitor and to gather this data or if there is a real upward rate in actual occurrence of these weather events.

This exercise was not intended to break any ground by creating any new awareness of what has been studied many times before this one.  The main purpose of my personal study was to gather an increased awareness of how to gather and how to complete a data mining study with some rather interesting data.

