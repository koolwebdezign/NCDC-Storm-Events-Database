---
title: "Data-Exploration"
author: "J. Welch"
date: "October 17, 2016"
output: 
  html_document: 
    keep_md: yes
---

# NCDC Storm Events Database Analysis

## Get and Clean Data from DATA.gov

I am an MBA student at Bridgewater State University, enrolled in MGMT582 - Business Intelligence/Analytics.  This data exploration project is intended to be my final project and in-class presentation.  The goal of this project is to gain practical working experience with R Studio and to apply some data analysis techniques in which we have been introduced within this course.

I have chosen to work with the NCDC Storm Events Database available at DATA.gov.  For more details related to this raw data set, see <https://catalog.data.gov/dataset/ncdc-storm-events-database>.

My first step in preparing my analysis will be to write a script to get the data at Data.gov.  After following instructions about the storm events database, I found that the raw data sits in zipped files located on an FTP server at <ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/>.  I am going to attempt to write a script that will automatically get this data from its FTP location, unzip each file, and then append it to create a larger file that can be analyzed here with R Studio.

```{r}
if (!file.exists("./stormdata.csv")) {
    # Load the R.utils library
    library("R.utils")
    
    i <- 1

    for (year in 2006:2016) {
        # Determine file name extension
        if (year==2014) {
            extstr <- "_c20160617.csv.gz"
        } else if (year==2015 || year==2016) {
            extstr <- "_c20160921.csv.gz"
        } else {
            extstr <- "_c20160223.csv.gz"
        }
        
        # Identify the URL of the download file
        URL <- paste("ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/StormEvents_details-ftp_v1.0_d", year, extstr, sep="")
        #print(URL)

        # Download the file into the current directory
        download.file(URL, destfile = "./temp.csv.gz")

        # Unzip the file
        gunzip("./temp.csv.gz")

        # Read the file into memory
        tempdata <- read.csv("./temp.csv")
        
        # Delete the lengthy narrative columns
        # Filesize with these columns is 272MB
        # Filesize without these columns 79MB
        tempdata$EPISODE_NARRATIVE <- NULL
        tempdata$EVENT_NARRATIVE <- NULL
    
        if (i==1) {
            stormdata <- tempdata
        } else {
            stormdata <- rbind(stormdata, tempdata)
        }

        # Delete the temporary file
        unlink("./temp.csv")
        
        # Write the loaded data to local CSV file
        write.csv(stormdata, "./stormdata.csv")
        
        # Increment i
        i <- i+1
    }
    
} else if (!exists("stormdata")) {
    stormdata <- read.csv("./stormdata.csv")
}

```
## Initial Table Exploration and Observation

The script above has successfully assembled 10+ years of data from the NCDC Storm Events Database from calendar year 2006 through summer of 2016.  We have eliminated the long text fields related to the description of the storm events.  We did this in order to produce a table size which is manageable on our desktop computers.  Elimination of these text fields dropped the file size to nearly 25% of its original size.  Our final CSV file is appx 193MB in size and it contains 656K observations of 50 variables.

It is now desirable to conduct some exploration techniques as outlined on RDatamining.com.  We will follow the examples put forth on the following page in order to begin our exploration of this significant data file.

<http://www.rdatamining.com/examples/exploration>


```{r}
# View the data frame inside the Global Environment of R Studio
#View(stormdata)

# Check the dimensionality
dim(stormdata)

# Output the names of the columns
names(stormdata)

# List the structure of the file
str(stormdata)

# Output the first 5 rows of the file
head(stormdata, n=5)

# Distribution of every variable
summary(stormdata)

# Frequency of event types
counts <- sort(table(stormdata$EVENT_TYPE), decreasing=TRUE)

# Get the names of columns on counts table
names(counts)

# Pie Chart
pie(counts)

# Simple Horizontal Bar Plot
barplot(counts, main="Storm Event Type", horiz=TRUE, xlab="Frequency", names.arg=names(counts), cex.names=0.5, las=1)

```

## Table Sub-Setting

The exercises in the *Initial Table Exploration and Observation* section allowed us to conduct some basic observations about the data set as a whole.  This is helpful yet when you browse the data and to gain a general understanding of the table content.  Now you may want to extract data into a subset (ie. to extract data by eliminating additional columns or to even extract information based on specific observations.)  Sub-setting is similar to strategies whereby one might execute a SQL SELECT statement from a database.  The difference here now with R is that our main data frame is sitting in memory.  We can now take pieces of this data set and conduct separate analysis.

```{r}

# Select by column
myvars <- c("YEAR", "INJURIES_DIRECT", "INJURIES_INDIRECT", "DEATHS_DIRECT", "DEATHS_INDIRECT", "DAMAGE_PROPERTY", "DAMAGE_CROPS")
impact <- stormdata[myvars]




```


